# Brain ðŸ§ 

A biologically plausible memory model that learns from text using **discrete synaptic states** and **local plasticity rules**. No numeric weights, no gradients, no backpropagationâ€”only mechanisms found in the biological brain.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Research](https://img.shields.io/badge/status-research-orange.svg)]()

**ðŸ“Š [Full Test Results & Baseline Comparison](docs/RESULTS.md)** â€” Brain vs TF-IDF/BM25: **+43-49% advantage**

---

## Quick Start

```bash
# Clone repository
git clone https://github.com/sss777999/brain.git
cd brain

# Install dependencies (using uv)
uv sync

# Run tests ((fast, without LLM and without GPT evaluation; baselines: TF-IDF, BM25 for QA, MemNet/NTM for bAbI)
uv run python test_brain.py --no-llm --no-gpt

# Run full tests with Broca's area (LLM verbalization) but without gpt evaluation
uv run python test_brain.py --no-gpt
```

### Requirements

- **Python 3.11+**
- **[uv](https://github.com/astral-sh/uv)** â€” fast Python package manager
- **[Ollama](https://ollama.ai/)** (optional) â€” for Broca's area verbalization (`gemma3:4b`)

### Train your own model

```bash
# Train on built-in curriculum (curriculum â†’ preschool â†’ grade1 â†’ FineWeb-Edu)
uv run python train.py

# Model saved to: models/brain_model_*.{npz,pkl}
```

### Use in code

```python
from train import ask, load_model

load_model()
answer = ask("What is the capital of France?")
print(answer)  # "paris"
```

---

## What makes this different?

| Traditional Neural Networks | Brain Model |
|----------------------------|-------------|
| Continuous weights (float32) | Discrete states: NEW â†’ USED â†’ MYELINATED â†’ PRUNE |
| Gradient descent + backprop | Local Hebbian learning + STDP |
| Vector embeddings | Neurons as discrete units |
| Attention matrices | Spreading activation + lateral inhibition |
| Fixed context window | Episodic memory + pattern completion |

**Key biological mechanisms implemented:**
- **STDP** â€” Spike-Timing Dependent Plasticity for directional connections
- **Four-factor learning** â€” STDP + eligibility traces + neuromodulators (DA/ACh/NE/5-HT)
- **Hippocampal circuit** â€” DG (pattern separation) â†’ CA3 (pattern completion) â†’ CA1 (output)
- **PFC working memory** â€” NMDA-like sustained activity, distractor resistance
- **Basal ganglia** â€” Action selection via Go/NoGo pathways
- **Sleep consolidation** â€” Sharp-wave ripples, forward/reverse replay

---

## How it answers questions

```
INPUT: "What is the capital of France?"
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. BROCA (broca.py): Parse question â†’ subject="france", connector="is_a" â”‚
â”‚ 2. PFC (pfc.py): Set goal, load context, classify question type          â”‚
â”‚ 3. BASAL GANGLIA (basal_ganglia.py): Select action (retrieve vs multi_hop)â”‚
â”‚ 4. ACTIVATION (activation.py): Spread through SEMANTIC connections        â”‚
â”‚    - MYELINATED paths conduct first, lateral inhibition, hub penalty     â”‚
â”‚ 5. HIPPOCAMPUS (hippocampus.py + ca3.py): Pattern completion             â”‚
â”‚    - CA3 attractor dynamics: spread â†’ WTA â†’ stability check              â”‚
â”‚    - Source filter: preferred + selective inclusion (Phase 21)           â”‚
â”‚    - Score: query overlap, connections, temporal bonus (P19), roles     â”‚
â”‚    - Connector: string Ã—5/Ã—0.2 (biased), frozenset Ã—2 (soft)           â”‚
â”‚    - Unconnected context filter, dedup top-K (Phase 20)                 â”‚
â”‚    - Best episode: ("capital", "france", "paris")                        â”‚
â”‚ 6. CA1 (ca1.py): Output layer, projects to PFC                           â”‚
â”‚ 7. MOTOR OUTPUT (motor_output.py): Filter question words â†’ ["paris"]     â”‚
â”‚ 8. LLM (optional): Grammatical verbalization â†’ "Paris"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
OUTPUT: "Paris"
```

**Spikes ARE used!** `_simulate_spike_pair()` in train.py creates spike_history, applies STDP with eligibility traces, and neuromodulators (DA/ACh/NE/5-HT) modulate plasticity. Connection imports EligibilityTrace, CalciumState, MetaplasticState from spiking.py.

---

## Key principles

### 1. Memory is not a "value", but a PATTERN

- A pattern is a set of neurons and connections that were frequently activated together
- We do NOT store a "value" as a number
- We store: "this pathway/ensemble â†’ pattern X"

### 2. Learning is not weight training, but pathway selection

- Frequently used pathways are stabilized (kept)
- Rarely used pathways disappear (pruning)
- Stable patterns emerge from repeated pathways

### 3. Activation is lightning

> "Activation passes like LIGHTNING in the sky â€” quickly, along the paths of least resistance"

- The signal goes FORWARD ONLY; there is no backward path
- Myelinated connections conduct faster (deep "grooves")
- Like a ball rolling along a carved landscape

### 4. Myelination is PRECISE KNOWLEDGE

- Not just "confidence after repetition"
- These are fundamental facts about reality
- Examples: "a cat meows" (a healthy cat does not bark), "you will fall if you jump from a height"

### 5. Knowledge expands with experience

- A lack of categories is normal when there is little knowledge
- An adult knows physics not because they read it 1000 times, but because they have enough experience
- The model accumulates knowledge gradually

### 6. Directed connections (STDP)

**STDP (Spike-Timing-Dependent Plasticity)** is a biological mechanism:
- If neuron B fires AFTER neuron A, the Aâ†’B connection is strengthened (forward)
- If neuron A fires AFTER neuron B, the Bâ†’A connection is strengthened (backward)

```python
class Connection:
    forward_usage: int   # Aâ†’B (A was before B)
    backward_usage: int  # Bâ†’A (B was before A)
```

Example:
- "cat meows" â†’ strengthens `catâ†’meows` (forward)
- "meows cat" â†’ strengthens `meowsâ†’cat` (forward for that order)

### 7. Connection limit (~7000 per neuron)

- In the real brain a neuron has ~7000 connections, not billions
- Connections are created as needed (Hebbian rule), not upfront
- When the limit is reached, old unused connections are removed

### 8. Word forms emerge naturally

- "cat" and "cats" are different neurons
- But if they often co-occur in text, the connection will strengthen
- No artificial lemmatization is neededâ€”the data will form connections

### 9. Tests describe REQUIRED BEHAVIOR

- Tests are NOT adjusted to match the code
- If a test fails, the problem is in the code, not in the test
- The real check is: train the network and verify what it recalls

---

## Current state

### âœ… What's done

| Component | Status | Description |
|-----------|--------|-------------|
| **CORE** | | |
| Neuron | âœ… | Binary state (active/inactive), no numeric weights |
| Connection | âœ… | Discrete states: NEW â†’ USED â†’ MYELINATED â†’ PRUNE |
| ConnectionType | âœ… | SEMANTIC (ventral) / SYNTACTIC (dorsal) â€” Dual Stream |
| Activation | âœ… | Propagation like "lightning" along connections |
| **SEMANTIC MEMORY** | | |
| Hebbian rule | âœ… | Connections are created during co-activation |
| STDP | âœ… | Connection directionality (forward_usage / backward_usage) |
| Myelination | âœ… | Consolidation of frequently used pathways |
| Chunking | âœ… | Merging frequent sequences |
| Inhibition | âœ… | Inhibitory neurons suppress weak branches |
| **EMERGENT HIERARCHY** | | |
| find_categories() | âœ… | Categories from graph topology (nodes with many incoming edges) |
| get_related_concepts() | âœ… | Related concepts by connection strength |
| NO IS_A/HAS_PROPERTY | âœ… | Hierarchy emerges implicitly, not explicitly |
| **BIOLOGICAL ATTENTION** | | |
| generate_with_attention() | âœ… | Generation with accumulating context |
| Decay | âœ… | Decay of old activations |
| Hub penalty | âœ… | log(1+n) â€” Weberâ€“Fechner law |
| Lateral inhibition | âœ… | Top-N strong activations suppress weaker ones |
| Winner-take-all | âœ… | Only winners remain active |
| Seed anchoring | âœ… | The topic (seed) always remains in memory |
| Working memory | âœ… | Limited capacity (~7 items) |
| **HOMEOSTATIC PLASTICITY** | | |
| Sparse coding | âœ… | ~2% active neurons in DG (Rolls et al., 2007) |
| Diluted connectivity | âœ… | Hebbian window of 4 words (not fully connected) |
| Heterosynaptic LTD | âœ… | Weak synapses weaken while strong ones strengthen |
| Synaptic Scaling | âœ… | Homeostatic plasticity, stable activity level |
| Competitive Learning | âœ… | Winner-Take-All in DG, experienced neurons win |
| Predictive Coding | âœ… | MYELINATED connections do not strengthen (already predictable) |
| **SPIKING NEURAL NETWORK** | | |
| Hodgkin-Huxley Model | âœ… | Biologically accurate membrane potential dynamics |
| Real STDP | âœ… | Spike-timing dependent plasticity based on spike_history |
| Ion Channels | âœ… | Na+, K+, Leak channels with gating variables m, h, n |
| Refractory Period | âœ… | Absolute (2ms) and relative (5ms) refractory period |
| Short-Term Plasticity | âœ… | Facilitation and Depression (Tsodyks-Markram model) |
| Dendritic Computation | âœ… | Proximal/Distal compartments with different integration |
| Metaplasticity | âœ… | Plasticity of plasticity (BCM rule) |
| Calcium Dynamics | âœ… | Ca2+-dependent plasticity |
| Three-Factor Learning | âœ… | Eligibility traces + neuromodulation |
| **NEUROMODULATION** | | |
| BrainOscillator | âœ… | Theta (6Hz) and Gamma (40Hz) oscillations |
| Dopamine System | âœ… | Novelty â†’ DA release â†’ boosted STDP |
| Acetylcholine | âœ… | Attention gate, modulates learning |
| Norepinephrine | âœ… | Arousal/surprise, increases excitability |
| Serotonin | âœ… | Behavioral inhibition, patience |
| **SOURCE MEMORY** | | |
| SourceType enum | âœ… | LEARNING / EXPERIENCE / CONVERSATION / MEDIA |
| QuestionType enum | âœ… | SEMANTIC_FACT / EXPERIENCE / LOCATION / TEMPORAL |
| Episode.trust | âœ… | Trust level based on source type |
| PFC routing | âœ… | classify_question() + get_preferred_sources() |
| CA3 filtering | âœ… | Selective inclusion: preferred always + MEDIA only if ALL query words match |
| Unconnected context filter | âœ… | Lateral inhibition: hard skip for structurally unconnected episodes |
| Source preference bonus | âœ… | Preferred-source episodes get additive scoring advantage |
| **CA3 ATTRACTOR DYNAMICS** | | |
| CA3 class | âœ… | Separate recurrent module for pattern completion |
| Iterative dynamics | âœ… | Spread activation + WTA + stability check |
| Full scoring | âœ… | 2-hop paths, context diversity, top-down modulation |
| **PFC PERSISTENT ACTIVITY** | | |
| NMDA slow decay | âœ… | tau ~100ms for sustained firing (Wang 2001) |
| Recurrent excitation | âœ… | Related slots reinforce each other |
| Distractor resistance | âœ… | GABAergic inhibitory gating (Miller & Cohen 2001) |
| Attractor dynamics | âœ… | Bistable states for stable activity |

### Current model (brain_model)

```
Training pipeline: curriculum â†’ preschool â†’ grade1 â†’ bAbI â†’ FineWeb-Edu (1000 articles, 40K sentences)
Neurons: 48,318
Connections: 1,471,243
MYELINATED: 23,792 (1.6%)
USED: 76,375 (5.2%)
NEW: 1,371,076
Episodes: 76,688
  - NEW: 35,086
  - REPLAYED: 2,185
  - CONSOLIDATED: 38,065
  - DECAYING: 1,352
```

**Test results** (07.02.2026):
```
CURRICULUM: 50/50 (100.0%) â€” hard tests
STRICT: 3/3 (100%) â€” tests for "I do not know"
PRESCHOOL: 48/48 (100.0%) â€” preschool tests
GRADE1: 64/64 (100.0%) â€” world-knowledge tests
FineWeb-Edu: 9/9 (100.0%) â€” direct facts from educational texts
PARAPHRASE: 50/50 (100.0%) â€” paraphrase robustness tests
bAbI Task 1: 250/250 (100%) â€” working memory tests
TOTAL: 474/474 (100.0%)
```

**Comparison with IR baselines** (same training data):
```
Test          Brain    TF-IDF   BM25     Brain advantage
CURRICULUM    100.0%   64.0%    70.0%    +30-36%
STRICT        100.0%   33.3%    33.3%    +66.7%
PRESCHOOL     100.0%   81.2%    87.5%    +12-19%
GRADE1        100.0%   68.8%    71.9%    +28-31%
FINEWEB       100.0%   11.1%    33.3%    +67-89%
PARAPHRASE    100.0%   48.0%    48.0%    +52.0%
bAbI Task 1*  100%     N/A      N/A      N/A
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AVERAGE       100.0%   51.1%    57.3%    +43-49%
```
*bAbI requires working memory â€” TF-IDF/BM25 cannot track entity states.

ðŸ“Š **[Full results with analysis](docs/RESULTS.md)**

**New mechanisms (February 2026):**
- **Broca's Area Phase 3 Reanalysis (PHASE 17)** â€” paraphrase normalization (Friederici 2011)
  - Transforms non-canonical question forms to canonical WH-questions
  - Inverted questions: "The sky is what color?" â†’ "What color is the sky?"
  - Imperative forms: "Name a farm animal" â†’ "What is a farm animal?"
  - Classifier stripping: "What kind of food is an apple?" â†’ "What is an apple?" (Croft 2001)
  - Passive constructions: "Cooking is done with what?" â†’ "What do we cook with?"
  - Possessive decomposition: "What is hot's opposite?" â†’ "What is opposite of hot?"
  - Temporal embedding: "What time of day do people wake up?" â†’ "When do people wake up?"
  - Result: PARAPHRASE 100.0% (was 50.0%)
- **Temporal Concept Inference (PHASE 19)** â€” on-the-fly temporal recognition (Eichenbaum 2014)
  - PFC sends "temporal" goal for 'when' questions â†’ primes temporal concept representations
  - Hippocampus checks if episode contains NEW temporal info (not already in query)
  - Combined with soft attentional facilitation (frozenset of before/after connectors)
  - Biology: anterior temporal lobe distinguishes temporal from spatial context
  - Result: all temporal questions now pass ("brush teeth"â†’day, "leaves fall"â†’autumn, "wash hands"â†’eating)
- **Episode Deduplication in Top-K (PHASE 20)** â€” consolidated memory merging (Born & Wilhelm 2012)
  - Multiple consolidated copies of same episode strengthen ONE attractor, not fill all top-K slots
  - Enables diverse secondary contributions from competing attractors via CA1 blending
  - Prevents echolalia when primary episode contains only query words
  - Result: sedimentary rock and paraphrase questions now pass
- **Source Memory Selective Inclusion (PHASE 21)** â€” biologically plausible retrieval hierarchy (Johnson et al. 1993)
  - Preferred sources (LEARNING, EXPERIENCE) always in candidate pool
  - Non-preferred sources (MEDIA) included ONLY when ALL content query words present in episode
  - Prevents MEDIA noise from overwhelming trusted sources while preserving domain-specific knowledge
  - Combined with unconnected context filter (lateral inhibition, Desimone & Duncan 1995)
  - "What disappears from leaves?" â†’ "green chlorophyll" (MEDIA selectively included)
  - "Who is the president of Mars?" â†’ "I do not know" (anti-hallucination preserved)
  - Result: **224/224 (100.0%)** â€” all 6 test suites at 100%
- **Hippocampal Time Cells for "When" Questions (PHASE 18)** â€” temporal retrieval (Eichenbaum 2014)
  - "When" as interrogative activates hippocampal time cells, biasing retrieval toward temporal info
  - Searches both 'before' and 'after' connections for temporal answers
  - Consolidation threshold: only consolidated connections (usage â‰¥ 1) are reliable (Born & Wilhelm 2012)
  - "When should you wash your hands?" â†’ "before eating"
  - Falls through to general retrieval when no temporal connections found

**New mechanisms (January 2026):**
- **Basal Ganglia Action Selection (PHASE 4)** â€” Go/NoGo/STN for strategy selection
  - D1 (Go) / D2 (NoGo) pathways in Striatum
  - GPi/GPe tonic inhibition, STN hyperdirect pathway
  - Neuromodulators (DA/ACh/NE/5-HT) modulate selection
  - Selection of "retrieve" vs "multi_hop" in `ask()`
- **TRUE SWR Replay (PHASE 6)** â€” Sharp Wave-Ripples with temporal compression
  - `_swr_event()` â€” generation of spike times with 15x compression
  - Forward replay: memory consolidation (BuzsÃ¡ki 2015)
  - Reverse replay (~30%): planning (Diba & BuzsÃ¡ki 2007)
  - NREM/REM phases with different replay mechanisms
  - Synaptic homeostasis: downscaling after sleep (Tononi & Cirelli 2006)
- **NMDA Receptor Mechanism** â€” dynamic threshold for context attention (Malenka & Bear 2004)
  - When strongly activated (â‰¥4 neurons) threshold decreases from 3 to 1
  - Weak synapses participate in Hebbian learning with high depolarization
  - Biology: MgÂ²âº block of NMDA receptor is removed at ~-40mV
- **Cross-Episode Linking** â€” semantic connections through shared context (McClelland et al. 1995)
  - During REM sleep, episodes with shared elements are replayed
  - Connections are formed between unique elements (dogâ†”cat through "animal")
  - Biology: Complementary Learning Systems â€” the hippocampus "teaches" the cortex
- **Source Memory (Johnson et al., 1993)** â€” the brain remembers WHERE knowledge came from
  - SourceType: LEARNING / EXPERIENCE / CONVERSATION / MEDIA
  - PFC classifies the question and routes to the appropriate sources
- **CA3 Attractor Dynamics** â€” biologically correct pattern completion
  - Iterative dynamics: spread activation + WTA + stability check
- **PFC Persistent Activity (PHASE 9.4)** â€” sustained activity in working memory
  - NMDA-like slow decay (tau ~100ms) for sustained firing (Wang 2001)
  - Recurrent excitation between related slots (attractor dynamics)
  - Distractor resistance via GABAergic inhibitory gating (Miller & Cohen 2001)
  - Goal-relevant inputs pass the barrier (top-down facilitation)
- **CA1 Output Layer (PHASE 9.2)** â€” the full hippocampal trisynaptic pathway
  - EC â†’ DG â†’ CA3 â†’ CA1 â†’ EC/PFC (Amaral & Witter 1989)
  - Schaffer collaterals (70%) + temporoammonic pathway (30%)
  - Projection to EC Layer V for consolidation and to PFC for working memory
- **Developmental Phases (PHASE 9.3)** â€” critical developmental periods
  - 4 stages: INFANT â†’ CHILD â†’ ADOLESCENT â†’ ADULT (Hensch 2005)
  - Critical periods for language/semantic/syntactic
  - Experience-expectant plasticity with learning bonuses
  - Synaptic pruning peaking in ADOLESCENT (Huttenlocher 1979)
- **Broca's Area / Syntactic Processing (PHASE 11)** â€” syntactic processing
  - SyntacticProcessor extracts subject/predicate from questions (Friederici 2011)
  - Subject bonus in CA3 scoring to prioritize relevant episodes
  - Binary choice: "Is winter cold or hot?" â†’ "cold"
- **Cause-Effect Relations (PHASE 12)** â€” cause-effect relations
  - Parsing questions of the form "What happens when X?"
  - CA3 filtering: the episode must contain the cause (subject)
  - Example: "What happens when ice gets warm?" â†’ "melts"
- **Temporal Sequence Fix (PHASE 13)** â€” temporal retrieval fix
  - Excluding question words from answer candidates
  - "What month comes after January?" â†’ "february" (not "month")
- **Antonym Relations (PHASE 14)** â€” biologically plausible antonym storage
  - Antonymy is encoded as connections with `connector='opposite'`
  - The same mechanism as temporal sequences (`connector='after'/'before'`)
  - Pattern "X is the opposite of Y" â†’ bidirectional connections Xâ†”Y
  - Works for ALL words including function words ("in"/"out")
  - "What is the opposite of in?" â†’ "out" (Murphy 2003)
- **Iterative Retrieval (PHASE 15)** â€” PFC-Hippocampus reasoning loop
  - `IterativeRetriever` class in `pfc.py` for multi-step reasoning
  - PFC maintains goal state, iteratively queries hippocampus
  - Each retrieval adds context to working memory (accumulation)
  - Confidence = goal overlap + consolidation bonus
  - Max 4 iterations (like humans â€” Eichenbaum 2017)
  - **Integrated into the main `ask()`**: when direct retrieval does not find an answer
  - Also used in `ask_multi_hop()` for explicit multi-step reasoning
  - Biology: Preston & Eichenbaum 2013, Miller & Cohen 2001
- **Semantic Roles (PHASE 16)** â€” event structure for goal-conditioned retrieval
  - Episodes store semantic roles: agent, patient, theme, cause, location, time, etc.
  - Based on Fillmore's Case Grammar (1968) and event semantics (Zacks & Tversky 2001)
  - 18 role types biologically grounded in temporal-parietal processing
  - `get_expected_roles()` â€” PFC determines expected roles based on question type
  - Goal-conditioned retrieval: "What is X?" â†’ category/property roles, "Where is X?" â†’ location role
  - Roles stored in Episode and serialized with model
- **Baseline Comparison** â€” scientific evaluation against standard IR methods
  - TF-IDF and BM25 baselines on the same curriculum data
  - Brain significantly outperforms: +40% vs TF-IDF, +50% vs BM25
  - Tests integrated: `--compare-baselines` flag in test_brain.py
- Hodgkin-Huxley spiking neurons with realistic membrane potential dynamics
- Real STDP based on spike timing
- **BrainOscillator** â€” theta/gamma oscillations
- **NeuromodulatorSystem** â€” dopamine, acetylcholine, norepinephrine, serotonin

**Examples of working questions (Brain raw â†’ Broca's area):**
| Question | Brain raw | Broca's area (LLM) |
|--------|-----------|-----------|
| What is a dog? | animal | An animal. |
| What color is the sky? | blue | blue |
| What is the capital of France? | paris | Paris |
| What does a cat say? | says meow meow | says meow meow |
| What comes after five? | six | Six comes after five. |
| What is the meaning of life? | love | Love is the meaning of life. |
| Who is the president of Mars? | I do not know | I do not know. |

**Why Broca's area (LLM)?**

Brain outputs **semantics**â€”a set of related words without grammar. This is the "thought" in its pure form.
The LLM (Qwen2.5:3b via Ollama) **verbalizes** the thought into speechâ€”similar to how Broca's area in the brain is responsible for speech production.

**Important:**
- The LLM does **NOT change facts**â€”it only adds grammar (articles, word order, punctuation)
- We **see both outputs** (Brain raw + Broca's area) for transparency and debugging
- Correctness is evaluated on **Brain raw**, not on Broca's areaâ€”the LLM can make grammatical mistakes, but facts always come from Brain

**What the model knows:**
- Colors of objects (skyâ†’blue, grassâ†’green, appleâ†’red)
- Animal sounds (dogâ†’bark, cowâ†’moo)
- Body parts (seeâ†’eyes, hearâ†’ears)
- Opposites (hotâ†’cold, bigâ†’small)
- Categories (dog+catâ†’animal, apple+bananaâ†’fruit)
- Emotions (laughâ†’happy, cryâ†’sad)
- Places (learnâ†’school, playâ†’park)

### âœ… Why 100% is NOT test-specific tuning

Each Phase 19â€“21 mechanism solves a **class of problems**, not a specific test case. None contains hardcoded words, question-specific thresholds, or answer lookups.

| Mechanism | Biological Basis | Generality |
|-----------|-----------------|------------|
| **Phase 19**: Temporal Concept Inference | Hippocampal time cells (Eichenbaum 2014). PFC top-down modulation (Miller & Cohen 2001). | ANY "when" question. 89-word temporal set (time-of-day, seasons, months, days, life stages). No question-specific logic. |
| **Phase 20**: Episode Deduplication | Consolidation merges traces into unified representations (Born & Wilhelm 2012). | ALL consolidated episodes. Generic `input_words` dedup â€” any episode with N copies â†’ 1. |
| **Phase 21**: Source Memory Selective Inclusion | Source memory = retrieval advantage, not gate (Johnson et al. 1993). Lateral inhibition (Desimone & Duncan 1995). | ALL questions with preferred sources. Generic `issubset()` check for non-preferred. Anti-hallucination preserved. |

**Free-form verification** (questions NOT in any test suite):
```
Q: Who is the king of Jupiter?      â†’ "I do not know"              âœ… anti-hallucination
Q: What is the capital of Germany?   â†’ "berlin..."                  âœ… LEARNING retrieval
Q: What is a cat?                    â†’ "animal and a pet that..."   âœ… standard retrieval
Q: When do children sleep?           â†’ temporal retrieval attempt    âœ… temporal inference
```

**Key criteria:**
1. **No hardcoded words** â€” temporal concepts are a general lexicon (89+ words), not test answers
2. **No question-specific logic** â€” all conditions are generic (`issubset()`, `input_words` dedup, role bonus)
3. **Anti-hallucination preserved** â€” novel nonsense questions correctly return "I do not know"
4. **Works on unseen data** â€” free-form questions answered from learned knowledge

### âš ï¸ Current limitations

1. **Word order in the answer** â€” Hippocampal Time Cells are implemented: episodes preserve word order (`input_words: Tuple`). When connections have equal priority, the episode order is used. LLM post-processing adds grammar.

2. **Scaling** â€” tested on 1000 FineWeb-Edu articles (40K sentences). Needs validation on larger datasets.

3. **Language Interpretation (Rule-Based Parsing)** âš ï¸
   
   The model uses **rule-based parsing** to interpret language, NOT learned linguistic knowledge:

   **âš ï¸ CRITICAL DISTINCTION: Grammar Coverage vs Fitting to Tests**

   | âŒ Fitting to tests (FORBIDDEN) | âœ… Expanding grammar coverage (ALLOWED) |
   |----------------------------------|-------------------------------------------|
   | Code works only for a specific test | Code handles a pattern that EXISTS in the curriculum |
   | Not in the data, added only to pass | The curriculum contains "hot and cold are opposites" â†’ the parser must understand it |
   | Hardcoded answer | Adding a grammar rule for a pattern present in the data |
   
   **Example:** The curriculum contains BOTH patterns:
   - "hot is the opposite of cold"
   - "hot and cold are opposites"
   
   The parser MUST support both. This is NOT fitting â€” it's **grammar coverage for existing data**.
   This corresponds to the theory of **Universal Grammar**: humans have innate syntactic structures.
   
   | Component | What it does | Why this is acceptable |
   |-----------|--------------|------------------------|
   | `broca.py` | Question patterns ("What is X?", "X and Y are opposites") | Models Universal Grammar |
   | `pfc.py` | Question classification by keywords | Category routing is biologically plausible |
   | `lexicon.py` | Lists of function words | Closed-class words are finite |
   | `motor_output.py` | Rules for inserting copulas | Models learned syntactic frames |
   | `train.py` | Pattern extraction (temporal, opposite, cause-effect) | Recognizes patterns from the curriculum |
   
   **Why this is done this way:**
   - The model is trained on ~1,000 basic sentences (plus 40K from FineWeb-Edu), not billions like an LLM
   - A child learns language from ~10M words by age 6â€”we do not have that volume of data
   - Rule-based parsing approximates what would be learned from a large body of language data
   
   **What IS learned (not rule-based):**
   - âœ… Semantic memory â€” associations via Hebbian learning
   - âœ… Episodic memory â€” storage and retrieval of events
   - âœ… Connection strength â€” MYELINATED through usage (STDP)
   - âœ… Pattern completion â€” CA3 attractor dynamics
   - âœ… Antonyms/temporal relations â€” learned from sentences, not hardcoded
   
   **Analogy:** Like a person who knows facts but uses a dictionary to translateâ€”the KNOWLEDGE is real, only the INTERFACE is simplified.

### ðŸ—„ï¸ Data storage (NumPy)

```
Format: NumPy arrays + a pickle dictionary
Load time: 0.6 seconds (2.3M connections)
Files:
  - graph_edges.npz â€” connections (src, dst, state, forward, backward, conn_type)
  - graph_vocab.pkl â€” vocabulary + connectors
```

**Connection format (STDP + Dual Stream):**
- `forward` â€” how many times `to` came AFTER `from`
- `backward` â€” how many times `from` came AFTER `to`
- `conn_type` â€” SEMANTIC (1) or SYNTACTIC (2)
- `connector` â€” a function word between content words (optional)

---

## How it works

### 1. Neuron (Hodgkin-Huxley Spiking Model)

```python
class Neuron:
    # Identification
    id: str                    # Unique identifier (word)
    neuron_type: NeuronType    # EXCITATORY / INHIBITORY
    
    # Hodgkin-Huxley dynamics
    V: float                   # Membrane potential (mV)
    m, h, n: float             # Ion-channel gating variables
    phase: NeuronPhase         # RESTING / DEPOLARIZING / REPOLARIZING / REFRACTORY
    spike_history: List[float] # Spike history for STDP
    
    # Connections
    connections_out: Set       # Outgoing connections
    connections_in: Set        # Incoming connections
```

**Biological model (Hodgkin & Huxley, 1952):**
- Membrane potential V changes under ionic currents (Na+, K+, Leak)
- Gating variables m, h, n control ion channel opening
- A spike is generated when V reaches the threshold (-55mV)
- After a spike there is a refractory period (absolute 2ms, relative 5ms)

**Biological constants:**
```python
V_REST = -70.0      # Resting potential (mV)
V_THRESHOLD = -55.0 # Spike threshold (mV)
V_PEAK = 40.0       # Action potential peak (mV)
E_NA = 50.0         # Na+ reversal potential (mV)
E_K = -77.0         # K+ reversal potential (mV)
```

### 2. Connection (with real STDP)

```python
class Connection:
    from_neuron: Neuron
    to_neuron: Neuron
    state: ConnectionState    # NEW / USED / MYELINATED / PRUNE
    forward_usage: int        # Pre before Post â†’ LTP
    backward_usage: int       # Post before Pre â†’ LTD
    
    # Real STDP based on spike timing
    def apply_stdp(self, current_time: float) -> None:
        """Applies STDP based on neurons' spike_history."""
        
    def propagate_spike(self, spike_time: float) -> None:
        """Propagates a spike to the postsynaptic neuron."""
```

**Biological STDP (Bi & Poo, 1998):**
- Pre before Post (dt > 0) â†’ **LTP** (Long-Term Potentiation) â€” strengthening
- Post before Pre (dt < 0) â†’ **LTD** (Long-Term Depression) â€” weakening
- The effect decays exponentially: `exp(-|dt| / tau)`, tau = 20ms

**Connection states:**
- `NEW` â€” new, unstable (0-4 uses)
- `USED` â€” strengthened (5-49 uses)
- `MYELINATED` â€” myelinated, precise knowledge (50+ uses)
- `PRUNE` â€” to be removed (unused for a long time)

**Thresholds (from config.py):**
```python
THRESHOLD_NEW_TO_USED = 5
THRESHOLD_USED_TO_MYELINATED = 50
THRESHOLD_TO_PRUNE = 100  # cycles without usage
```

### 3. Neuromodulation System (NEW!)

```python
class NeuromodulatorSystem:
    dopamine: float      # Reward/novelty signal
    acetylcholine: float # Attention gate
    norepinephrine: float # Arousal/surprise
    serotonin: float     # Behavioral inhibition
    
    def release(modulator, amount)  # Neuromodulator release
    def get_learning_rate_modifier() # Learning-rate modifier
    def get_excitability_modifier()  # Excitability modifier
```

**Biology (Schultz 1998, Gerstner 2018):**
- **Dopamine** â€” novelty/reward signal, boosts STDP for new connections
- **Acetylcholine** â€” attention gate, opens "gates" for learning
- **Norepinephrine** â€” arousal/surprise, increases neuronal excitability
- **Serotonin** â€” behavioral inhibition, patience

**Dopamine during learning:**
```
New connection â†’ is_novel=True â†’ _release_dopamine(0.3) â†’ DAâ†‘ (0.1â†’0.4)
â†’ da_modifier = 1.0 + (DA - 0.1) * 2 = 1.6
â†’ eligibility.value *= da_modifier â†’ enhanced LTP
```

### 4. Brain Oscillator

```python
class BrainOscillator:
    theta_freq: float = 6.0   # Hz (episodic memory)
    gamma_freq: float = 40.0  # Hz (local computation)
    
    def update(dt_ms) â†’ (theta, gamma)
    def get_excitability() â†’ float  # Modulation from theta phase
```

**Biology (Buzsaki 2006):**
- **Theta (4-8 Hz)** â€” hippocampus, episodic memory, navigation
- **Gamma (30-100 Hz)** â€” binding, attention, local computation
- **Theta-Gamma Coupling** â€” sequence encoding

### 5. Activation

Activation spreads like "lightning":

```
Step 1: cat (start)
        â†“ âš¡ (MYELINATED)
Step 2: meows
```

**Neuron activation conditions:**
1. Receives a signal via a MYELINATED connection â€” activates immediately
2. Receives signals from 2+ active neighbors via USED connections â€” co-activation

### 4. Hebbian rule

"Neurons that fire together, wire together"

```python
# When learning from the sentence "cat meows":
conn = Connection.get_or_create(cat, meows)
conn.mark_used()  # usage_count += 1
```

Connections are created at first co-activation and strengthened with repetition.

### 5. Pattern

A pattern is a set of connected neurons that activate together.

```
        meows
           âš¡
           â”‚
fluffy   â•â•âš¡â•â• CAT  â•â•âš¡â•â• pet
           â”‚
           âš¡
        animal
```

---

## Files

```
Brain/
â”œâ”€â”€ neuron.py              # Hodgkin-Huxley spiking neuron
â”œâ”€â”€ connection.py          # Connection with real STDP (spike timing)
â”œâ”€â”€ activation.py          # Activation propagation + spike simulation
â”œâ”€â”€ spiking.py             # Full spiking module (STP, Dendritic, Metaplasticity)
â”œâ”€â”€ hippocampus.py         # Episodic memory (DG, CA3, SWR)
â”œâ”€â”€ cortex.py              # Semantic memory (Pattern storage)
â”œâ”€â”€ config.py              # Single config for all model parameters
â”œâ”€â”€ llm_postprocess.py     # LLM post-processing (Broca's area)
â”œâ”€â”€ train.py               # Training with STDP and Q&A
â”œâ”€â”€ curriculum.py          # Curriculum data (facts for a 5-year-old)
â”œâ”€â”€ pattern.py             # Pattern class, patterns
â”œâ”€â”€ episode.py             # Episodic memory (Episode class)
â”œâ”€â”€ pyproject.toml         # Dependencies (uv/pip)
â””â”€â”€ tests/
    â””â”€â”€ test_brain.py      # Tests (curriculum, grade1, fineweb)
```

---

## What matches the specification (plan.md)

### âœ… FORBIDDEN and complied with:

| Restriction | Status |
|------------|--------|
| Numeric connection weights | âœ… No |
| Metrics/distances (cosine, dot) | âœ… No |
| Optimization (gradients, backprop) | âœ… No |
| Global search (Dijkstra, BFS) | âœ… No |
| Probabilistic models (softmax) | âœ… No |
| Deep Learning layers | âœ… No |
| Embedding as a meaning vector | âœ… No |

### âœ… ALLOWED and implemented:

| Mechanism | Status |
|----------|--------|
| Local connection history (usage_count) | âœ… |
| Discrete states | âœ… |
| Activation as lightning | âœ… |
| Pattern as a set of neurons | âœ… |
| Hebbian rule | âœ… |
| Connection limit (~7000) | âœ… |

---

## Roadmap

### âœ… PHASE 1: Semantic memory [DONE]
- Connections between concepts via Hebbian learning
- Myelination of frequent pathways (STDP)
- Spreading activation
- Chunking as an emergent property
- Dual Stream: SEMANTIC + SYNTACTIC

### âœ… PHASE 2: Emergent hierarchy [DONE]
- NO explicit IS_A/HAS_PROPERTY â€” this is not biologically plausible
- Categories emerge from the structure of connections
- find_categories() â€” category discovery from graph topology
- get_related_concepts() â€” related concepts by connection strength

### âœ… PHASE 2.5: Biological attention [DONE]
- generate_with_attention() â€” generation with context
- ACCUMULATIVE CONTEXT: each word adds its neighbors
- DECAY: old activations decay
- HUB PENALTY: log(1+n) â€” Weberâ€“Fechner law
- LATERAL INHIBITION: top-N strong activations suppress weaker ones
- WINNER-TAKE-ALL: only winners remain
- SEED ANCHORING: the topic always stays in memory
- Working memory: ~7 items

### âœ… PHASE 2.6: Curriculum training [DONE]
- Training on basic facts (like a 5-year-old child)
- Tests: 10/10 (100%)
- Biological mechanisms fully implemented

### âœ… PHASE 3: Episodic memory [DONE]
- Hippocampus as a temporary buffer for new events
- DG (Dentate Gyrus) â€” pattern separation (sparse coding, ~2%, Rolls et al. 2007)
- CA3 â€” pattern completion (reconstruction from a partial cue)
- Episodes store input_neurons for retrieval
- Consolidation via replay

### âœ… PHASE 3.5: Attention during Retrieval [DONE]
- **Question context** is preserved during pattern_complete
- **query_overlap** â€” prioritizes episodes containing the original question words
- **avg_strength** â€” average strength of queryâ†’answer connections (myelinated pathways)
- **Activation history** â€” the full history is used, not only the final state
- The same mechanisms work both during training and inference

### âœ… PHASE 3.6: Interrogative Words + LLM Postprocess [DONE]
- **INTERROGATIVE_WORDS** â€” a separate class (what, where, who, when, why, how)
  - Create neurons and participate in activation
  - Do NOT form connections among themselves (like function words)
  - BIOLOGY: activate an "expectation template" in the prefrontal cortex
- **NO connector normalization** â€” "is", "was", "are" are stored as-is (biologically plausible)
- **Temperature** â€” probabilistic episode selection (like softmax in GPT)
- **config.py** â€” a single config for all model parameters
- **LLM Postprocess** (llm_postprocess.py) â€” Broca's area
  - Brain outputs semantics: "dog is animal"
  - The LLM formats into speech: "A dog is an animal."
  - The LLM does **NOT change facts**, only grammar

### âœ… PHASE 3.7: Anti-Hallucination (Context Word Connectivity) [DONE]
- **Problem:** The model answered "Who is the president of Mars?" â†’ "president of country is leader"
- **Solution:** A biologically grounded check of context-word connectivity to the episode
  - Context words = query words that are NOT in the episode
  - If a context word is not connected to any word in the episode â†’ the episode is irrelevant
  - Example: "mars" is not connected to {president, country, leader} â†’ skip â†’ "I do not know"
- **BIOLOGY:** The hippocampus rejects memories that are not activated by the input signal
- **Result:** 100% on hard tests (53/53), including "I do not know" for nonsensical questions

### âœ… PHASE 3.8: Top-Down Modulation + VERB_FORMS [DONE]
- **Top-Down Modulation** (Zanto et al. 2011)
  - `connector_filter` in Activation â€” prioritizes connections with a matching connector
  - For the question "What IS X?" connections with `connector="is"` are activated
  - `query_connector` in pattern_complete â€” +50 bonus for matching connector
  - **BIOLOGY:** PFC modulates retrieval by task type
- **Context Diversity** (Spens & Burgess 2024)
  - Counter of distinct episodes in which a connection occurred
  - Connections from diverse contexts are more semantic
- **Multi-hop Context**
  - CA3 looks 2 steps ahead to understand context
  - Recurrent connections in CA3 for pattern completion
- **VERB_FORMS** â€” morphological verb forms
  - `fall/falls/fell/falling`, `give/gives/gave/giving`, etc.
  - Query expansion to search episodes with different forms
  - **BIOLOGY:** The brain links different forms of the same word
- **Result:** Grade1 64/64 (100%)

### âœ… PHASE 4: Basal Ganglia Action Selection [DONE]
- Go/NoGo/STN for cognitive strategy selection
- D1 (Go) / D2 (NoGo) pathways in Striatum
- GPi/GPe tonic inhibition, STN hyperdirect pathway
- Integrated into `ask()`: selection of "retrieve" vs "multi_hop"
- Biology: Cortex â†’ Striatum â†’ GPi/GPe â†’ Thalamus â†’ Cortex

### âœ… PHASE 6: TRUE REPLAY / SWR [DONE]
- Sharp Wave-Ripples with temporal compression (15x)
- Forward replay: memory consolidation (BuzsÃ¡ki 2015)
- Reverse replay (~30%): planning (Diba & BuzsÃ¡ki 2007)
- SleepPhase enum: WAKE / NREM / REM
- NREM: SWR replay + slow oscillations
- REM: random reactivation for integration
- Synaptic homeostasis: downscaling after sleep

### âœ… PHASE 7: Internal Actions [DONE]
- BG selects cognitive actions: RETRIEVE / MULTI_HOP / INFER / WAIT
- Working Memory / Semantic Memory / Episodic Memory routing
- Integrated with PFC for routing

### ðŸŸ¡ PHASE 8: Learn VERB_FORMS [NEXT]
- Remove hardcoded `VERB_FORMS` dict
- Morphology via learning (like children)
- Links goesâ†”went via shared context

### âšª PHASE 9: Additional Improvements
- DG Pattern Separation without hash()
- Sparse coding: 5:1 compression, WTA
- Scaling to 50K+ articles

### ðŸŸ¢ Improvements

- **Pruning at the connection limit** â€” automatic removal of old connections
- **Multimodality** â€” visual input, modality binding

---

## How to run

### Installation
```bash
git clone https://github.com/sss777999/Brain.git
cd Brain
uv sync  # or pip install -e .
```

### Training on FineWeb-Edu
```bash
# Test (100 articles, ~10 seconds)
PYTHONPATH=. python train.py

# Full training (10K articles, ~30-40 minutes)
# Change in train.py: max_articles=10000, max_sentences=500000
```

### Brain model tests
```bash
python3 test_brain.py              # ALL tests (curriculum + grade1)
python3 test_brain.py --curriculum # Curriculum-only tests
python3 test_brain.py --grade1     # Grade 1 tests only
python3 test_brain.py --train      # Train a single model (curriculum â†’ grade1)
python3 test_brain.py --strict     # Hard tests with correctness checks
python3 test_brain.py --raw        # Without LLM post-processing
```

### Model training
```bash
python3 test_brain.py --train      # Full pipeline: curriculum â†’ grade1 â†’ brain_model
python3 train.py full              # Alternative training method
```

### Loading the model in Python
```python
from graph_storage import GraphStorage
storage = GraphStorage.load('graph')
print(storage.get_neighbors('science', min_state=1)[:10])
```

---

## Project files

| File | Purpose |
|------|---------|
| `neuron.py` | Neuron class (binary state) |
| `connection.py` | Connection class with STDP (forward/backward) |
| `activation.py` | Activation logic (lightning over connections) |
| `graph_storage.py` | NumPy graph storage (fast loading) |
| `train.py` | Training on FineWeb-Edu |

### Saved models

| File | Description |
|------|-------------|
| `graph_edges.npz` | Connections (src, dst, state, forward, backward) |
| `graph_vocab.pkl` | Word vocabulary |

---

## Principles

1. **Biological plausibility** â€” everything as in the brain, without artificial computations
2. **Locality** â€” a neuron knows only its neighbors; there is no global observer
3. **Discreteness** â€” states, not numbers
4. **Natural selection** â€” frequently used connections strengthen, rare ones die off
5. **Patterns** â€” memory = connection structure, not values

---

## Metaphor

> Memory is like a landscape with grooves.
> Activation is like a ball rolling along those grooves.
> Myelinated connections are deep grooves.
> The ball rolls where the strengthened paths lead.

---

## Memory types (LLM architecture)

### Semantic memory (âœ… implemented)
- World knowledge: "a cat meows", "the sun is a star"
- Not tied to time/place
- Stored in the cortex (cortex)

### Episodic memory (âœ… implemented)
- Hippocampus as a temporary buffer for new events (`hippocampus.py`)
- **DG (Dentate Gyrus)** â€” pattern separation (sparse coding, ~2%)
- **CA3** â€” pattern completion (reconstruction from a partial cue)
- **SWR (Sharp Wave-Ripples)** â€” replay and consolidation during sleep
- Episodes store `input_words` (word order) for correct generation
- Consolidation via `sleep()` â€” strengthening connections and myelination
- 64,013 episodes in the current model (26,160 CONSOLIDATED)

```
Episodic memory (hippocampus)
    â†“ consolidation (replay)
Semantic memory (cortex)
```

### Cause-effect relations (ðŸ”´ needed)
- "pressed the button" â†’ "the light turned on"
- For reasoning, not for memory
- Requires understanding of time and agency

### Grammar/syntax (ðŸ”´ needed)
- Word order in a sentence
- For text generation
- The next step after memory

---

## STRICTLY FORBIDDEN (from the plan.md specification)

| Forbidden | Why |
|----------|-----|
| Numeric connection weights (0.37, 0.85) | A connection exists or not; at most it has qualitative states |
| Metrics and distances (cosine, dot product) | You cannot choose a pattern by "minimum distance" |
| Optimization (gradients, backprop, loss) | There is no "error as a number"; only stabilization or decay |
| Global search (Dijkstra, BFS, A*) | Activation propagates locally; a neuron knows only its neighbors |
| Probabilistic models (softmax, Bayes) | Randomness only as a source of chaos, not as a knowledge model |
| Deep Learning layers (Linear, ReLU, Transformer) | Structures can be used for storage, but not as carriers of meaning |
| Symbolic rules (if A and B then C) | Logic can exist in control code, but memory is not stored as rules |
| Embedding as a meaning vector | Embedding is only a packaging of structure, not a geometric object |

---

## Data volume estimation

### Thresholds (from config.py)
- NEW â†’ USED: 5 repetitions
- USED â†’ MYELINATED: 50 repetitions

### How much data is needed
```
For one word pair (catâ†’meows):
  - Need 50 sentences where both words occur together
  
For 100 basic facts:
  - Need ~50,000 sentences

This matches real learning:
  - A child hears "cat meows" hundreds of times
  - Before it becomes stable knowledge
```

### Current status
- Synthetic dataset: works (repetitions are manually specified)
- Real dataset: 580 sentences (too few, connections do not reach thresholds)
- Needed: real texts (Wikipedia, books, FineWeb)


---

## How meaning is formed

### Meaning = the pattern of connections around a word

```
                    meows
                      âš¡
                      â”‚
    fluffy   â•â•âš¡â•â• CAT  â•â•âš¡â•â• pet
                      â”‚
                      âš¡
                   animal
                      â”‚
                      â†’
              mammal
                      â”‚
                      â†’
                    lion (also a feline!)
```

### Different phrasings strengthen the SAME connections

```
"The cat is fluffy" â†’ connection catâ†”fluffy +1
"A fluffy cat" â†’ connection catâ†”fluffy +1 (the same connection!)
"The cat is soft and fluffy" â†’ connection catâ†”fluffy +1
```

After 50 repetitions: `cat â•â•âš¡â•â•> fluffy` (MYELINATED)

### What is NOT overwritten

- Connections only strengthen or weaken
- New information adds new connections
- Repeated information strengthens existing ones
- Unused connections â†’ PRUNE (forgetting)

---

## Pattern visualization

### Example: query "einstein"

```
ðŸ“ STEP 1: einsteinâš¡ â†’ theory, relativity
ðŸ“ STEP 2: theoryâš¡ â†’ darwin, evolution
ðŸ“ STEP 3: darwin + einstein â†’ scientist
ðŸ“ STEP 4: scientistâš¡ â†’ physicist, biologist
ðŸ“ STEP 5: physicist + scientist â†’ newton

ðŸ§  FINAL PATTERN:
âš¡ Precise knowledge: darwin, relativity, theory, scientist, physicist, evolution
â†’  Associations: biologist, newton, evolution
```

### Legend
- `â•â•âš¡â•â•>` â€” myelinated connection (precise knowledge)
- `â”€â”€â”€â”€â”€â”€>` â€” USED connection (association)
- `+` â€” co-activation from multiple sources

---

## Decision history

### Why connections in both directions?
- Initially we thought: only forward in word order
- But: "meows" should activate "cat" (backward association)
- Decision: connections in both directions as independent synapses

### Why not lower thresholds?
- Temptation: lower thresholds for a small dataset
- But: that is artificial, not like the brain
- Decision: increase the data, do not lower thresholds

### Why not lemmatization?
- Temptation: artificially treat "cat" = "cats" = "cat (acc.)"
- But: in the brain these are different forms linked through experience
- Decision: learn it naturally from data

### STDP (temporal dynamics) â€” IMPLEMENTED!
- **Spike-Timing-Dependent Plasticity** â€” biological mechanism
- Connections now store `forward_usage` and `backward_usage`
- Word order matters: "cat meows" â‰  "meows cat"
- This enables generating text in the correct order

### Hebbian rule and window size
> **"Neurons that fire together wire together"** â€” Donald Hebb, 1949

Hebbian rule: connections form between neurons that are **co-active in time**.

#### Biologically grounded window size

The original Hebbian rule does not define a "window size". We derive it from biology:

| Parameter | Value | Source |
|----------|-------|--------|
| Reading speed | ~250ms per word | Psycholinguistics |
| Working memory | ~2000ms (~7Â±2 items) | Miller's law, 1956 |
| Hebbian time window | ~100ms | Neuroscience (STDP) |

**Calculation (diluted connectivity, Rolls et al. 2007):**
```
Diluted connectivity: 4 words (HEBBIAN_WINDOW_SIZE)
```

**Window = 4 words** â€” sparser connectivity increases memory capacity and reduces interference.

#### How it works

When reading text, words activate sequentially:
- Words within window (4 words) form connections
- Words beyond that are too far â†’ NO connection

```
"The cat sat on the mat"

Hebbian rule (window = 4, diluted connectivity):
  cat â†” sat âœ“ (within window â€” connection)
  cat â†” on âœ“ (within window â€” connection)
  cat â†” mat âœ— (beyond window â€” NO connection)
```

**This is biologically plausible:** Connections form between words that a person holds in consciousness at the same time.

---

## Stop-word handling

### Principle
- Stop words (prepositions, conjunctions, pronouns) participate in connections with other words
- Stop words do NOT create connections among themselves
- During recall, stop words are filtered out from results

### Example
```
Sentence: "a book lies on the table"

Connections created:
  âœ“ on â†’ table (stop + content)
  âœ“ table â†’ lies (content + content)
  âœ“ lies â†’ book (content + content)
  âœ— on â†’ and (stop + stop â€” skip)

When recalling "table":
  â†’ lies, book (stop words filtered out)
```

### Why this way?
- In the brain, a child hears "on the table", "under the table" and learns that "on/under" change meaning
- But by themselves "on", "under" without context are meaningless
- Connections to them are needed, but they should not dominate results

---

## Lateral inhibition (Lateral Inhibition)

### Biological mechanism
In the brain, strongly activated neurons suppress weakly activated neighboring neurons via inhibitory interneurons.

**Implementation:**
1. Top-N strongest activations are "winners" (not inhibited)
2. The rest receive inhibition proportional to the gap from the leader
3. The weaker relative to max, the stronger the inhibition

```python
# Code in graph_storage.py process_sentence()
max_score = scored[0][1]
top_n_inhibitors = 5  # Winners

for i, (word, score) in enumerate(scored):
    if i < top_n_inhibitors:
        # Top-N are not inhibited
        inhibited_scores.append((word, score))
    else:
        # Inhibition proportional to the difference from max
        relative_strength = score / max_score
        inhibition_factor = 0.5 + 0.5 * relative_strength
        final_score = score * inhibition_factor
```

### Hub Penalty (Weberâ€“Fechner law)
Neurons with many connections have a higher activation threshold:

```python
hub_penalty = 1.0 / math.log1p(num_neighbors + 1)
```

This is biologically plausible: hubs (common words) are less specific and require more input signal to activate.

---

## Numbers and dates handling

### Principle
- Digits are preserved: "1945", "2024", "15th" â€” these are important data
- Pure numbers ("10", "100") are filtered during recall (too much noise)
- Numbers with letters ("15th", "2024th") remain

### Why not remove digits?
- Dates are knowledge: "1945 â€” the end of the war"
- Numbers in context matter: "100 rubles", "15 kilometers"
- Decision: keep them in the graph; filter pure numbers during recall

---

## Storage architecture (NumPy)

### Why NumPy?
| Solution | Speed | Complexity | Scalability |
|---------|-------|------------|-------------|
| Pickle (old) | âŒ Slow | âœ… Simple | âŒ Poor |
| **NumPy arrays** | âœ… Fast | âœ… Simple | âœ… Good |
| SQLite | âš ï¸ Medium | âš ï¸ Medium | âœ… Good |
| Neo4j | âœ… Fast | âŒ Complex | âœ… Excellent |

### Data structure
```python
{
    "word_to_id": {"russia": 0, "moscow": 1, ...},  # dict
    "id_to_word": ["russia", "moscow", ...],        # list
    "edges_src": np.array([0, 0, 1, ...]),          # int32
    "edges_dst": np.array([1, 2, 3, ...]),          # int32
    "edges_state": np.array([2, 1, 2, ...]),        # int8 (0=NEW, 1=USED, 2=MYELINATED)
    "edges_usage": np.array([50, 10, 55, ...]),     # int32
    "neighbors": {0: [(1, 2, 50), ...], ...}        # index for fast lookup
}
```

### Does not violate project principles
- NumPy is a storage format, not a computation model
- Like a book vs an e-book: the content is the same, the medium is different
- The activation logic remains biologically plausible

---

## Scientific analysis: what we are doing in the context of AI history

### How LLMs were created

**2017: Transformer (Vaswani et al.)**
- Idea: attention instead of recurrence
- They did not know whether it would scale
- They just tried it

**2018-2020: GPT-1 â†’ GPT-2 â†’ GPT-3**
- They observed: more parameters = better quality
- Scaling laws (Kaplan et al., 2020): a predictable relationship
- The revolution: "just scale it up and it will work"

**Key point:** They did not understand WHY it worked. They simply scaled up and observed correlation.

---

### What we are doing â€” an honest analysis

**Similarities with early LLMs:**
1. We also do not know for sure whether it will scale
2. We also rely on a hypothesis (biological plausibility = the right path)
3. We need to increase data and observe correlation

**Differences:**
1. LLMs are an empirical approach ("scale and see")
2. We are a theoretical approach ("do it like the brain")

---

### Honest assessment of our approach

**Strengths:**

1. **Biological foundation** â€” the brain works, so the principle is valid
2. **Interpretability** â€” we see patterns and understand why
3. **Efficiency** â€” O(kÃ—s) instead of O(nÂ²Ã—d)
4. **Incremental learning** â€” no need to retrain the whole model

**Weaknesses / unknowns:**

1. **Text generation** â€” the brain generates speech via other mechanisms (Broca's area); we do not model this yet
2. **Scaling** â€” not validated at millions of neurons
3. **Quality** â€” not compared against LLMs on real tasks

---

### What science says

**Neuroscience:**
- Memory in the brain really works via strengthening connections (Hebb, 1949)
- Myelination is real and speeds signal conduction
- Hippocampus â†’ cortex is a real consolidation mechanism

**Scale:**
- The brain has ~86 billion neurons
- We model ~1000 neurons
- A difference of 86 million times

**Question:** Will emergent properties appear with scaling?

LLMs showed: yesâ€”new capabilities emerge with scaling (in-context learning, reasoning).

---

### Where we are now

**Analogy with LLMs:**
- GPT-1 was weak but showed the direction
- GPT-2 showed that scaling works
- GPT-3 was a breakthrough

**We are currently at the "GPT-1" stage** â€” proof of concept is done; we need to scale.

---

### Next step â€” a large-scale experiment

1. Take a real dataset (Russian Wikipedia, ~2 million articles)
2. Train the model (millions of sentences)
3. Measure:
   - How many connections become MYELINATED?
   - What patterns form?
   - Does recall work on complex queries?

**If this shows strong results, we have grounds for a revolution.**
**If not, we will learn what needs to change.**

> This is what LLM creators did: they scaled and observed.

---

## Current status (January 2026)

### Trained model statistics
```
Pipeline: curriculum â†’ preschool â†’ grade1 â†’ FineWeb-Edu (1000 articles, 40K sentences)
Neurons: 48,301
Connections: 1,453,469
MYELINATED: 19,252 (1.3%)
USED: 77,745 (5.3%)
NEW: 1,356,472
Episodes: 68,947 (30,748 CONSOLIDATED, 2,139 REPLAYED)
```

### Test results (23.01.2026)
```
CURRICULUM: 49/50 (98.0%)
STRICT: 3/3 (100%)
PRESCHOOL: 46/48 (95.8%)
GRADE1: 64/64 (100%)
FineWeb-Edu: 7/9 (77.8%)
bAbI: 250/250 (100%)
TOTAL: 419/424 (98.8%)
```

### Usage

**Tests (without retraining):**
```bash
python3 test_brain.py --no-gpt --no-llm --skip-babi  # Fast tests
python3 test_brain.py                                 # All tests with LLM
```

**Model training:**
```bash
python3 test_brain.py --train  # Full training pipeline
```

---

## Future plan: Morpheme segmentation

### When to introduce
- After scaling (10,000+ articles)
- When recall shows morphology issues

### What to do
```python
import pymorphy2
morph = pymorphy2.MorphAnalyzer()

# "cat" â†’ ["cat", ""]
# "cats" â†’ ["cat", "s"]
# A shared root can create connections
```

### Biological plausibility
- The brain has areas for morphemes (fusiform gyrus, ~130ms)
- Morphemes are minimal units of meaning
- Enables understanding new words: "reboot" = [re][boot]

---

## Citation

If you use this work in your research, please cite:

```bibtex
@article{belyi2026brain,
  title={Brain: Structural Memory, Thought Formation, and Language in a Biologically Grounded System},
  author={Belyi, Vitalii},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2026},
  url={https://github.com/sss777999/Brain}
}
```

Full paper: [docs/arxiv.pdf](docs/arxiv.pdf)

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| **v1.0** | Jan 24, 2026 | Initial release. 98.8% accuracy (419/424 tests). Hippocampus, PFC, Basal Ganglia, Broca's area, STDP, sleep consolidation. |

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## Contributing

This is an open research project. Contributions, suggestions, and collaborations are welcome!

- **Issues**: Report bugs or suggest features
- **Pull requests**: Code improvements
- **Discussions**: Ideas about biological plausibility, new mechanisms

Contact: [GitHub Issues](https://github.com/sss777999/Brain/issues)
