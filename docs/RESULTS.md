# Brain Model Test Results

**Date:** February 05, 2026 (auto-generated)
**Model:** brain_model
**Training:** curriculum → preschool → grade1 → bAbI → FineWeb-Edu

---

## Model Statistics

| Metric | Value |
|--------|-------|
| Neurons | 48,295 |
| Connections | 1,477,365 |
| MYELINATED | 19,195 (1.3%) |
| USED | 77,936 (5.3%) |
| NEW | 1,380,234 |
| Episodes | 68,945 |
| — NEW | 35,150 |
| — REPLAYED | 2,142 |
| — CONSOLIDATED | 30,744 |
| — DECAYING | 909 |

---

## Test Results Summary

| Test Suite | Passed | Total | Accuracy | Description |
|------------|--------|-------|----------|-------------|
| **CURRICULUM** | 49 | 50 | **98.0%** | Core knowledge tests |
| **STRICT** | 3 | 3 | **100.0%** | "I do not know" tests |
| **PRESCHOOL** | 46 | 48 | **95.8%** | Ages 3-6 knowledge |
| **GRADE1** | 64 | 64 | **100.0%** | Grade 1 world knowledge |
| **FINEWEB** | 7 | 9 | **77.8%** | Educational text facts |
| **PARAPHRASE** | 25 | 50 | **50.0%** | Surface form robustness |
| **TOTAL** | **194** | **224** | **86.6%** | All tests combined |

---

## Comparison with IR Baselines

All baselines trained on **identical data** (curriculum.py sentences + connections).

| Test | Brain | TF-IDF | BM25 | Brain vs TF-IDF | Brain vs BM25 |
|------|-------|--------|------|-----------------|---------------|
| CURRICULUM | **98.0%** | 58.0% | 48.0% | **+40.0%** | **+50.0%** |
| STRICT | **100.0%** | 33.3% | 33.3% | **+66.7%** | **+66.7%** |
| PRESCHOOL | **95.8%** | 22.9% | 22.9% | **+72.9%** | **+72.9%** |
| GRADE1 | **100.0%** | 39.1% | 37.5% | **+60.9%** | **+62.5%** |
| FINEWEB | **77.8%** | 0.0% | 0.0% | **+77.8%** | **+77.8%** |
| PARAPHRASE | **50.0%** | 38.0% | 38.0% | **+12.0%** | **+12.0%** |
| **AVERAGE (QA)** | **86.9%** | **31.9%** | **30.0%** | **+55.0%** | **+57.0%** |

### Key Findings

1. **Brain significantly outperforms simple IR methods** (+55-77%)
3. **Paraphrase robustness** — 50% accuracy indicates room for improvement
4. **"I don't know" capability** — Brain correctly abstains on unknown queries


---

## Failed Tests Analysis

### CURRICULUM (1 failures)
| Question | Brain Answer | Expected |
|----------|--------------|----------|
| What is ice? | melts when it gets warm | melts when it gets warm |

### PRESCHOOL (2 failures)
| Question | Brain Answer | Expected |
|----------|--------------|----------|
| What happens when you fall? | I do not know | I do not know |
| When should you wash your hands? | eat | eat |

### FINEWEB (2 failures)
| Question | Brain Answer | Expected |
|----------|--------------|----------|
| What disappears from leaves? | I do not know | ['chlorophyll', 'green'] |
| What is sedimentary rock made of? | sedimentary rock is made | ['bones', 'shells', 'organic', 'sandstone', 'limestone', 'shale'] |

### PARAPHRASE (25 failures)
| Question | Brain Answer | Expected |
|----------|--------------|----------|
| A dog is what kind of thing? | things smell and some good and some smell bad | ['animal', 'pet', 'mammal'] |
| Dogs belong to what category? | I do not know | ['animal', 'pet', 'mammal'] |
| Tell me what a dog is | I do not know | ['animal', 'pet', 'mammal'] |
| What category does a dog belong to? | I do not know | ['animal', 'pet', 'mammal'] |
| Apples are classified as what? | I do not know | ['fruit'] |
| What kind of food is an apple? | I do not know | ['fruit'] |
| Tell me the sky's color | I do not know | ['blue'] |
| What is hot's opposite? | I do not know | ['cold'] |
| Name the capital of France | beijing china | ['paris'] |
| Paris is located where? | I do not know | ['france'] |

---

## How to Reproduce

```bash
# Train model
python train.py

# Run all tests with baseline comparison
python test_brain.py --no-gpt --no-llm

# Run specific test suite
python test_brain.py --curriculum --no-gpt --no-llm
```

---

*This file is auto-generated by `test_brain.py`. Do not edit manually.*
